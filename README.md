<h1>Applied Data Science Portfolio (IST 782)</h1>

Below is the overview, video submission, and blog post for the IST 782 requirment. To view projects (also required for the project), please navigate to my [project portfolio](https://github.com/kharmer9/).

<h3>[See Video Overview HERE!](https://youtu.be/RB0Tdj4KC3E)</h3>

<h3>Academic Learning Outcomes</h3>

<h5><i>Objective 1.  Collect, store, and access data by identifying and leveraging applicable technologies</i></h5>

Every data science pipeline involves this goal in the initial phases. Generally, once an objective is determined, data scientists look for data to extract or begin considering how to acquire data that will help them acquire some actionable insight. This can be achieved in multiple ways, which include but is not limited to surveying, web-scraping, and transactional tracking (like cookies). Then, the data storage must be stored efficiently on a physical system, assuming the data scientist does not have data available through a first-party (like company data) or third-party (like MongoDB) database. While common computer storage devices are sufficient for storing data, big data is a growing field that requires support from other data-storing entities (like hfds or spark). Once the data has been collected and stored, data scientists can consider using it. However, there are multiple methods of accessing data, which include common software like excel to programming software like python or R. Big data requires further accessibility consideration like pyspark (spark extension of python). Once data is stored effectively, data scientists start their analysis process to obtain actionable insight.

<h5><i>Objective 2.	Create actionable insight across a range of contexts (e.g. societal, business, political), using data and the full data science life cycle</i></h5>

Data is being shared in society today like never before. From financial reports to social media algorithms, even to the health metrics measured by smartwatches, data has developed into a major commodity across the business world. The industry shift to data-based solutions has left many questioning how a series of numbers or text descriptions improve business operations. The data, however, can reveal much about a problem, regardless of the field. Generalized descriptive statistics can provide an easily obtainable overview of the metrics involved with data. Amazon, for example, can use product ratings to determine which of their products are popular. By identifying exemplary products, Amazon can shift their business strategies to maximize their products. Further data exploration other than descriptive statistics, like trend searching and distribution analysis, can also provide insight. Trend analysis is prevalent in climate change analysis, where it demonstrates how the climate changes and the rate at which it has been changing, leaving an impact on societal viewpoints. Distribution analysis, on the other hand, can be seen in credit card defaults, where it can identify outliers in diverse fields that flag people who will potentially default on a payment. Identifying these consumers in advance can help the distributor give them the help they need to stay on pace. Although these analyses are valuable and important to the overall contribution of the data, their contributions are minimal in comparison to predictive modeling. Predictive modeling is a strong tool for tech giants like Google, Amazon, Tesla, and frankly any other tech company. Chat GPT, the new natural language processor which does research across the web to answer generalized questions, is a predictive model that Microsoft invested $1 billion because they knew how beneficial their data modeling would be to the industry. The world is shifting to data-based solutions due to the widespread insight they can act on.

<h5><i>Objective 3.	Apply visualization and predictive models to help generate actionable insight</i></h5>

Although actionable insight can be acquired in many ways, the most common way to acquire or demonstrate insight is through visualization and predictive models. The best way to explain a relationship or a distribution of one or more entities is through a visual plot, chart, or picture. For example, explaining which places experienced the most COVID cases per capita during the pandemic could be explained by a heatmap (map with color chart representing severity of COVID cases) much better than listing off the top countries. Predictive models are even more efficient; they are designed to acquire specific output which can be preset to the insight trying to be obtained, giving another factor to make business decisions with. Companies across the globe use predictive models but take the banking industry for example. The banking industry relies on credit and loans to make their money by accumulating profits through the interest rates present in money borrowed by consumers. However, defaulting payments are all too common in the industry, thus raising the question of who should they loan money to and how much. Predictive modeling can identify who will default on their loan or payments based on their demographic information, like their monthly income, monthly expenses, etc. Defaulting payments leads directly to actionable insight related to credit limits and projected payment defaults, thus giving the industry a crucial tool for running their business. Predictive models, along with visualization, are essential to not only a data scientist, but any modernized industry.

<h5><i>Objective 4.	Use programming languages such as R and Python to support the generation of actionable insight</i></h5>

As mentioned, programming languages are essential to generating any actionable insight with data science. While a statistical and mathematical background is important, modeling tools in applied statistical programming are the most important part of data science. Model algorithms and visualization techniques are made conveniently possible with the development of computers in recent years, allowing statisticians to skip the intense calculations associated with modeling and significantly improve the design of visualizations. Without the development of these tools, actionable insight would be significantly more difficult to come by through data science.

<h5><i>Objective 5.	Communicate insights gained via visualization and analytics to a broad range of audiences (including project sponsors and technical team leads)</i></h5>
  
As hinted at in the previous objective, many people that make important decisions in business or otherwise do not have the technical background to be able to make decisions based on data. The software requirements previously mentioned, along with the statistical background and the time/resources required to generate actionable insights are not generally completed by the business executives. Instead, as expected, the work is left to data scientists. However, problems vary in complexity and insights generally have different amounts of value based on their output. So, along with being able to identify those insights, being able to explain findings and effectively communicate to a variety of audiences are essential to data science.

With the proper background, effective visualizations can be generated which influence the knowledge of technical leads, project sponsors, and in some cases, a generalized audience. Furthermore, appropriate analytics must be completed to precisely solve certain problems based on business recommendations. Overall, without the ability to effectively communicate the results of work, a data scientist could not be overly beneficial to their field.

<h5><i>Objective 6.	Apply ethics in the development, use, and evaluation of data and predictive models (e.g., fairness, bias, transparency, privacy)</i></h5>
  
Perhaps the largest barrier to data science in today's age is the ethical implications. With the development of widespread data, many consumers and authority figures are questioning how data is obtained without violating privacy policies. In recent years, tighter restrictions (e.g. Apple's "Ask App not to Track") have been placed restricting the flow of data. Privacy does not stop with the flow of data; predictive models have begun to make predictions about important, private parts of people's lives, some of which they do not know. Target, for example, had an advertising campaign that was able to predict a woman's pregnancy (through advertisements of diapers, baby clothes, etc.) based on her buying habits where the woman did not even know she was pregnant. Predictive models struggle with more than just privacy on occasion â€“ specific biases, including but not limited to racial, sexist, and age-related have influenced the projection of models (e.g. loan default). Considering these factors is very important when constructing a model. It is important to be fair across all demographics, regardless of data trends.

<h3>Program Reflection</h3>

Overall, the program was just what I needed to begin a career with data. I came in with a background with mathematics and physics, with some computer science knowledge (practical applications in math/physics). Covid was also rampant at the time and many companies had restricted hiring procedure. So, studying data science for two years was a perfect fit, due to the concentrated area of study along with giving the job market some time to reset.

As for my program expectations specifically, I was interested largely in the coding requirements. I came in with some data science experience in python and R, but did not have an understanding strong enough to begin a career. So, I wanted to learn more about applicable advanced algorithms for data science modeling, mainly in python. I expected to learn more about machine learning methods and how machine models are commonly designed and used in industry. I also hoped to learn more about artificial neural networks and how to develop them.

Aside from python, I expected to learn more about the general uses of R. I heard before attending Syracuse that it was a statistical programming language which was being slowly phased out in the workplace. So, I expected to learn a functional amount of R and some methods that are commonly used in the workplace using R.

Along with modeling tools, I wanted to learn more about how to automate modern processes. In data science, I was looking to learn a more efficient way to obtain and preprocess data. The initial phases of the data science process can be difficult and/or take a large portion of time, so I was trying learn how to improve efficiency here to focus more time on the mathematical modeling. But after learning more about the data science process, I realized how difficult this can be. The ADS program gave me an outstanding starting point with its teachings on databases and data cleaning.

The program had mixed affects on my personal learning goals. I learned much about programming in python, while also extending the knowledge into pyspark. Although I did not work much with neural networks, I got plenty of experience with various machine learning methods that gave me some insight on how to model different types of data. This is directly in line with the second learning objective and demonstrated by my project portfolio, which is linked at the top of this page. Furthermore, I learned a lot about text data and how to mine/process the data into useful models.

My expectations of R were met, but primarily through my summer internship. While I did take a few classes in R, I felt the ADS curriculum only scratched the surface on what R could do. My 2022 summer internship showed me just how powerful the language is, especially when programming with respect to statistic standards. R, along with practices in python, helped me fulfill the fourth learning objective. Aside from statistical analysis, the ADS program showed me another crucial tool to data science: Data Visualization. The program taught me much about data visualization and how influential can be to data science projects. It also showed me how to make complex visualizations, especially in R, thus fulfilling the third learning objective of the program.

Lastly, the program provided me with experience and expertise working with databases and data engineering. I had never used SQL going into the program, and now I feel nearly as skillful using SQL as python or R. I feel this level is perfect due to the different variations of SQL being used in scattered databases. It leaves me with just enough to compare to others, while also not being set into one way of accomplishing a database task. Although I may not have experience with every available database, I feel I have the skill required to learn new databases and database languages quickly, therefore fulfilling the first learning objective.

As for program specifics, I liked the natural langauge processing class (IST 654: Natural Langauage Processing). Going into the class, I had a very weak understanding of linguistics, especially how it could be performed using data. It provided a eye-opening understanding of the world and how much data is actually available. Furthermore, it had the most complicated coding requirements (expect maybe pyspark in IST 718 or database command line in IST 769) which was great because I felt intellectually challenged.

Yet despite the course content, my favorite part of the course was the [final project](https://github.com/kharmer9/NLP_Neural_Network/blob/main/README.md). It was my only class where I worked with neural networks and it was my chance to develop one of my one. I ended up designing a model which read in material from any author available on gutenberg.org (I picked Mark Twain), then wrote a new story using the data available from the stories. Not only did it spark my interests, but it showed me how influential an author is when writing a language processor. Mark Twain was from a different time where language and perspectives were very different from those today. Seeing his style of writing and vocabulary implemented into a new model really projected how biases underlying in data can affect results. Although my classes did have ethics modules, I did not really understand how important it was until seeing the different possible results in this project (which fulfilled the sixth learning objective for me).

The best part of the program for me was the project-based curriculum. I feel that data science can only be truly appreciated with the depth of projects, meaning it needs aspects from all different parts of the data science life cycle. Only then can one appreciate clean data or data accessibility while also understanding and appreciating the results of predictive models and visualizations. Furthermore, with a diverse set of projects, I gained the opportunity to acquire a deep amount of information in several different areas. Then, I got to explain complex findings in several instances, helping me grow my communication skills and fulfilling the fifth learning outcome. Also, it helped me develop the project portfolio linked above.
